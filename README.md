# Logistic Regression – Regularization & Overfitting 

This assignment is an application of logistic regression for binary classification, covering gradient descent, decision boundaries, and regularization to reduce overfitting.  
It is part of **Week 3 (Course 1: Supervised Machine Learning – Regression and Classification)** from the **Machine Learning Specialization** by Andrew Ng on Coursera.

##  Description

In this practice lab, I implemented **logistic regression** to classify data using a sigmoid function and optimize the cost with gradient descent. The assignment also demonstrates the issue of **overfitting** and how to mitigate it using **L2 regularization**.

### Key Concepts Covered:
- Logistic regression cost function
- Sigmoid activation and binary classification
- Gradient descent for parameter optimization
- Overfitting vs underfitting
- L2 regularization to improve generalization
- Decision boundary visualization

##  Files Included

- `logistic_regression_lab.ipynb`: Main notebook with code and explanations
- `lab_utils_multi.py`: Utility functions for plotting, cost, and accuracy
- `data/`: Includes training and testing datasets

> ⚠️ This repository includes only my own code and results, and respects Coursera's Honor Code.

##  Tools Used

- Python 3
- NumPy
- Matplotlib
- Jupyter Notebook

##  Course Info

This assignment is part of:
> [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)  
> Instructor: **Andrew Ng**  
> Course 1: Supervised Machine Learning – Regression and Classification  
> Week 3: Overfitting and Regularization in Logistic Regression

## License

This repository is for learning and portfolio purposes only. Please do not copy it for assignment submissions.

---

# If you're learning ML, feel free to explore or star this repo!
